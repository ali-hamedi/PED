{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530e2550",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\n",
    "\n",
    "*A deep dive into implementing NeRF from scratch. We start by setting up the environment and understanding the mathematical foundations of Ray Marching.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8615c7f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 0: Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b97654b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "### basics\n",
    "\n",
    "from os import path\n",
    "import json\n",
    "\n",
    " \n",
    "### base \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9b772",
   "metadata": {},
   "source": [
    "## Step 1: The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180aab2f",
   "metadata": {},
   "source": [
    "### 1.1 Source\n",
    "\n",
    "We are using the **Synthetic Lego Dataset** \n",
    "\n",
    "  * **Download:** [Google Drive Link](https://drive.google.com/drive/folders/1cK3UDIJqKAAm7zyrxRYVFJ0BRMgrwhh4)\n",
    "  * **wget:** wget https://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/nerf_example_data.zip\n",
    "### 1.2 Directory Structure\n",
    "\n",
    "After extraction, ensure your folder looks like this:\n",
    "\n",
    "```text\n",
    "nerf_synthetic/\n",
    "  └── lego/\n",
    "      ├── train/                # Training Images\n",
    "      ├── val/                  # Validation Images\n",
    "      ├── test/                 # Test Images\n",
    "      ├── transforms_train.json # Camera Poses for training\n",
    "      ├── transforms_val.json\n",
    "      └── transforms_test.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881419ae",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Camera Metadata (`transforms.json`)\n",
    "\n",
    "The JSON file contains the **Intrinsic** (Focal Length) and **Extrinsic** (Position/Rotation) parameters for every image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46af5760",
   "metadata": {},
   "source": [
    "### 2.1 Focal Length & Intrinsic Matrix ($K$)\n",
    "\n",
    "At the top of the JSON, `camera_angle_x` gives us the Field of View (FOV). We use this to calculate the Focal Length ($f$) and build the Intrinsic Matrix ($K$).\n",
    "\n",
    "  * **`camera_angle_x`**: Horizontal FOV in radians.\n",
    "  * **`W`, `H`**: Image Width and Height.\n",
    "\n",
    "$$f = \\frac{W / 2}{\\tan(\\text{camera\\_angle\\_x} / 2)}$$\n",
    "\n",
    "**The Intrinsic Matrix ($K$):**\n",
    "This maps 3D camera coordinates to 2D image pixels.\n",
    "\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "f & 0 & W/2 \\\\\n",
    "0 & f & H/2 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "![Image of pinhole camera focal length field of view diagram](focal-angle.png)\n",
    "\n",
    "### 2.2 Camera Poses (Extrinsics)\n",
    "\n",
    "For every frame, the `transform_matrix` represents the **Camera-to-World ($c2w$)** transformation.\n",
    "\n",
    "```json\n",
    "\"frames\": [\n",
    "    {\n",
    "        \"transform_matrix\": [\n",
    "            [r11, r12, r13, tx],\n",
    "            [r21, r22, r23, ty],\n",
    "            [r31, r32, r33, tz],\n",
    "            [0.0, 0.0, 0.0, 1.0]\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "**The Pose Matrix ($T_{c2w}$):**\n",
    "This places the camera into the 3D world.\n",
    "\n",
    "$$\n",
    "T_{c2w} = \\left[ \\begin{array}{c|c}\n",
    "\\mathbf{R} & \\mathbf{t} \\\\\n",
    "\\hline\n",
    "0 & 1\n",
    "\\end{array} \\right]\n",
    "\n",
    "$$\n",
    "\n",
    "  * **$\\mathbf{R}$ (Rotation $3 \\times 3$):** Defines the camera's orientation (Right, Up, Back vectors).\n",
    "  * **$\\mathbf{t}$ (Translation $3 \\times 1$):** Defines the camera's position $(x, y, z)$ in the world.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e3de3",
   "metadata": {},
   "source": [
    "## Step 3: Ray Generation Math\n",
    "\n",
    "To render the scene, we must convert 2D pixels $(u, v)$ into 3D Rays $\\mathbf{r}(t) = \\mathbf{o} + t\\mathbf{d}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776cf8b0",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 The Three Coordinate Spaces\n",
    "\n",
    "1.  **Pixel Space $(u, v)$**: The 2D grid of the image.\n",
    "2.  **Camera Space $(x_c, y_c, z_c)$**: 3D coordinates relative to the camera lens.\n",
    "3.  **World Space $(x_w, y_w, z_w)$**: The absolute 3D coordinates of the scene.\n",
    "\n",
    "### 3.2 The Algorithm (Pixel $\\rightarrow$ Ray)\n",
    "\n",
    "**Step A: Pixel to Camera Space**\n",
    "We project the 2D pixel to a 3D direction vector. Note the negative $Z$ (forward) and negative $Y$ (screen coordinates vs 3D coordinates).\n",
    "\n",
    "$$\n",
    "x_c = \\frac{(u - W/2)}{f}, \\quad y_c = \\frac{-(v - H/2)}{f}, \\quad z_c = -1\n",
    "$$\n",
    "\n",
    "**Step B: Camera Space to World Space**\n",
    "We use the `transform_matrix` (Pose) to rotate and translate this vector into the world.\n",
    "\n",
    "  * **Ray Origin ($\\mathbf{o}$):** The camera's position.\n",
    "      * `rays_o = transform_matrix[:3, 3]`\n",
    "  * **Ray Direction ($\\mathbf{d}$):** The pixel vector rotated by the camera's orientation.\n",
    "      * `rays_d = transform_matrix[:3, :3] @ [x_c, y_c, z_c]`\n",
    "\n",
    "### 3.3 Practical Correspondence Table\n",
    "\n",
    "| Concept | In Math / Code | In JSON Data |\n",
    "| :--- | :--- | :--- |\n",
    "| **Focal Length** | $f$ | Calculated from `camera_angle_x` |\n",
    "| **Rotation** | $R$ (Orientation) | `transform_matrix[:3, :3]` |\n",
    "| **Translation** | $t$ (Position) | `transform_matrix[:3, 3]` |\n",
    "| **Ray Origin** | $\\mathbf{o}$ | Matches **Translation** exactly |\n",
    "| **Ray Direction** | $\\mathbf{d}$ | Calculated using **Rotation** and $f$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bdc7e0",
   "metadata": {},
   "source": [
    "## Step 4: Create Dataset And DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44848153",
   "metadata": {},
   "source": [
    "### Step 4.1:  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "881d1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegoRayDataset(Dataset):\n",
    "    def __init__(self,base_path='./nerf_synthetic/lego',split='train'):\n",
    "        self.base_path = base_path\n",
    "        self.images = path.join(base_path,split)\n",
    "\n",
    "        json_file_path = path.join(base_path, f'transforms_{split}.json')\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            self.transforms = json.load(f)\n",
    "        self.fov = self.transforms['camera_angle_x']\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.transforms['frames'])\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.transforms['frames'][idx]\n",
    "        \n",
    "        fname = frame['file_path']\n",
    "        if fname.startswith('./'):\n",
    "            fname = fname[2:] \n",
    "        \n",
    "        image_path = path.join(self.base_path, f\"{fname}.png\")\n",
    "\n",
    "        img = plt.imread(image_path) \n",
    "        img = torch.from_numpy(img).float()\n",
    "\n",
    "        if img.shape[-1] == 4:\n",
    "            rgb = img[..., :3]\n",
    "            alpha = img[..., 3:4]\n",
    "            \n",
    "            img = rgb * alpha + (1 - alpha)\n",
    "        \n",
    "        pose = torch.tensor(frame['transform_matrix'], dtype=torch.float32)\n",
    "        return img,pose,self.fov\n",
    "    \n",
    "def get_rays(H, W, fov, c2w):\n",
    "    focal = 0.5 * W / np.tan(0.5 * fov)\n",
    "\n",
    "    i, j = torch.meshgrid(torch.arange(W, dtype=torch.float32), \n",
    "                        torch.arange(H, dtype=torch.float32), indexing='xy')\n",
    "    # 0 1 2 ....   W-1\n",
    "    # 1\n",
    "    # 2\n",
    "    # 3\n",
    "    # .\n",
    "    # .\n",
    "    # .\n",
    "    # H-1        H-1 W-1   \n",
    "    \n",
    "    device = c2w.device\n",
    "    i = i.to(device)\n",
    "    j = j.to(device)\n",
    "\n",
    "    dirs = torch.stack([(i - W * .5) / focal, -(j - H * .5) / focal, -torch.ones_like(i)], -1)\n",
    "    #Cartesian\n",
    "    \n",
    "\n",
    "\n",
    "    #  i - W*0.5: Centers the coordinates. \n",
    "    # -(j - H*0.5): Centers the Y-axis.\n",
    "        # Why negative? In images, \"Down\" is +Y. In 3D graphics (OpenGL/NeRF), \"Up\" is +Y\n",
    "    # / focal: Normalizes the pixel distance. \n",
    "\n",
    "    # -1: The Z-direction.\n",
    "    # By convention, the camera looks down the Negative Z-axis.\n",
    "    \n",
    "\n",
    "    # d_world​= R ⋅ d_camera​\n",
    "    rays_d = torch.sum(dirs.unsqueeze(-2) * c2w[:3, :3], -1) \n",
    "    # dirs shape  (H, W,  3) -->(H, W, 1, 3) [[x,y,z]] (a row that can be multiplied by a matrix) \n",
    "    # input: (H, W, 3, 3)\n",
    "    # sum over last axis (3).\n",
    "    # output: (H, W, 3).\n",
    "    rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "    rays_o = c2w[:3, 3].expand(rays_d.shape)\n",
    "    # c2w shape (3,) ==> expand (H,W,3)\n",
    "    \n",
    "    return rays_o, rays_d  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1895f531",
   "metadata": {},
   "source": [
    "### Step 5.1:  DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d3df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ray_collate_fn(batch):\n",
    "    all_rays_o = []\n",
    "    all_rays_d = []\n",
    "    all_target_colors = []\n",
    "    \n",
    "    for img, pose, fov in batch:\n",
    "        H, W = img.shape[:2]\n",
    "        \n",
    "        N_RAYS_PER_IMAGE = 4096\n",
    "\n",
    "        coords = torch.stack(torch.meshgrid(\n",
    "            torch.arange(H, dtype=torch.float32),\n",
    "            torch.arange(W, dtype=torch.float32),\n",
    "            indexing='ij'\n",
    "        ), -1).reshape(-1, 2)\n",
    "        # HWx2\n",
    "        # [[0,0],\n",
    "        #  [0,1],\n",
    "        #  [0,2],\n",
    "        #  ...\n",
    "        #  [H-1, W-1]]\n",
    "\n",
    "\n",
    "        \n",
    "        select_inds = np.random.choice(coords.shape[0], size=[N_RAYS_PER_IMAGE], replace=False)\n",
    "        #select_inds = [501, 12, 9999, 42, ...]\n",
    "\n",
    "        select_coords = coords[select_inds].long() \n",
    "        \n",
    "        i = select_coords[:, 1] \n",
    "        j = select_coords[:, 0] \n",
    "        \n",
    "        focal = 0.5 * W / np.tan(0.5 * fov)\n",
    "        \n",
    "        dirs = torch.stack([(i - W * .5) / focal, -(j - H * .5) / focal, -torch.ones_like(i)], -1)\n",
    "        \n",
    "        rays_d = torch.sum(dirs[..., np.newaxis, :] * pose[:3, :3], -1)\n",
    "        rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "        rays_o = pose[:3, 3].expand(rays_d.shape)\n",
    "        \n",
    "        target_colors = img[j, i] \n",
    "\n",
    "        all_rays_o.append(rays_o)\n",
    "        all_rays_d.append(rays_d)\n",
    "        all_target_colors.append(target_colors)\n",
    "\n",
    "    return torch.cat(all_rays_o, 0), torch.cat(all_rays_d, 0), torch.cat(all_target_colors, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62d45068",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LegoRayDataset(split='train')\n",
    "val_dataset = LegoRayDataset(split='val')\n",
    "test_dataset = LegoRayDataset(split='test')\n",
    "\n",
    "BATCH_SIZE=2\n",
    "train_loader = DataLoader(train_dataset,BATCH_SIZE,shuffle=True,collate_fn=ray_collate_fn,num_workers=8,prefetch_factor=16,persistent_workers=True,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset,BATCH_SIZE,shuffle=True,collate_fn=ray_collate_fn,num_workers=8,prefetch_factor=16,persistent_workers=True,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset,BATCH_SIZE,collate_fn=ray_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfdf6de",
   "metadata": {},
   "source": [
    "## Step 6: NeRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2470b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X : location; \n",
    "L = 10\n",
    "# d : direction\n",
    "L=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3d62ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942461d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e075fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_input, n_freqs, ):\n",
    "        super().__init__()\n",
    "        self.d_input = d_input\n",
    "        self.n_freqs = n_freqs\n",
    "        self.d_output = d_input * (1 + 2 * n_freqs) \n",
    "        self.embed_fns = [lambda x: x] \n",
    "\n",
    "        freq_bands = 2.**torch.linspace(0., n_freqs - 1, n_freqs)\n",
    "        for freq in freq_bands:\n",
    "            self.embed_fns.append(lambda x, freq=freq: torch.sin(x * freq * 3.14159))\n",
    "            self.embed_fns.append(lambda x, freq=freq: torch.cos(x * freq * 3.14159))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([fn(x) for fn in self.embed_fns], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1afa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeRF(nn.Module):\n",
    "    def __init__(self, L1=10, L2=4):\n",
    "        super().__init__()\n",
    "        self.embed_pos = PositionalEncoder(3, L1)\n",
    "        self.embed_dir = PositionalEncoder(3, L2)\n",
    "        \n",
    "        input_ch_pos = self.embed_pos.d_output \n",
    "        input_ch_dir = self.embed_dir.d_output\n",
    "\n",
    "        self.density_mlp1 = nn.Sequential(\n",
    "            nn.Linear(input_ch_pos, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.skip_layer = nn.Linear(256 + input_ch_pos, 256)\n",
    "\n",
    "        self.density_mlp2 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.density_head = nn.Linear(256, 1)\n",
    "        self.feature_head = nn.Sequential( \n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.color_mlp = nn.Sequential(\n",
    "            nn.Linear(256 + input_ch_dir, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3) \n",
    "        )\n",
    "\n",
    "    def forward(self, x, d):\n",
    "        x = x / 4.0 \n",
    "\n",
    "        x_embedded = self.embed_pos(x)\n",
    "        d_embedded = self.embed_dir(d)\n",
    "        \n",
    "        h = self.density_mlp1(x_embedded)\n",
    "        \n",
    "        h = torch.cat([h, x_embedded], dim=-1)\n",
    "        \n",
    "        h = self.skip_layer(h)\n",
    "        \n",
    "        h = self.density_mlp2(h)\n",
    "        \n",
    "        sigma = self.density_head(h)\n",
    "        \n",
    "        feature = self.feature_head(h)\n",
    "        h_color = torch.cat([feature, d_embedded], dim=-1)\n",
    "        \n",
    "        rgb = torch.sigmoid(self.color_mlp(h_color)) \n",
    "        \n",
    "        return sigma, rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1193350f",
   "metadata": {},
   "source": [
    "## Step7 : Setup the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c70cc21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4001  Epochs\n"
     ]
    }
   ],
   "source": [
    "coarse = NeRF().to(device)\n",
    "fine = NeRF().to(device)\n",
    "params = list(coarse.parameters()) + list(fine.parameters())\n",
    "\n",
    "optimizer = optim.Adam(params, lr=5e-4,  betas=(0.9, 0.999))\n",
    "\n",
    "TARGET_STEPS = 200000 \n",
    "steps_per_epoch = len(train_loader) # 100/BATCH_size\n",
    "\n",
    "num_epochs = int(TARGET_STEPS / steps_per_epoch) + 1\n",
    "print(num_epochs,\" Epochs\")\n",
    "# num_epochs=200\n",
    "\n",
    "gamma = (5e-5 / 5e-4) ** (1 / num_epochs)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def volume_render(raw, z_vals, rays_d):\n",
    "    # raw: [N_rays, N_samples, 4] \n",
    "    # z_vals: [N_rays, N_samples] \n",
    "    # rays_d: [N_rays, 3] \n",
    "    \n",
    "    # delta\n",
    "    dists = z_vals[..., 1:] - z_vals[..., :-1] \n",
    "    \n",
    "    # The last sample goes to infinity\n",
    "    last_dist = torch.tensor([1e10], device=raw.device).expand(dists[..., :1].shape)\n",
    "    \n",
    "    dists = torch.cat([dists, last_dist], -1)\n",
    "\n",
    "    # distance = dt\n",
    "    dists = dists * torch.norm(rays_d.unsqueeze(1), dim=-1)\n",
    "\n",
    "    # rgb and sigma\n",
    "    rgb = raw[..., :3] \n",
    "    sigma = F.relu(raw[..., 3])        \n",
    "\n",
    "    # opacity\n",
    "    opacity = 1.0 - torch.exp(-sigma * dists)\n",
    "\n",
    "    # T\n",
    "    p = 1.0 - opacity + 1e-10\n",
    "    T = torch.cumprod(torch.cat([torch.ones((opacity.shape[0], 1), device=raw.device), p], -1), -1)[:, :-1]\n",
    "\n",
    "    # weights\n",
    "    weights = T * opacity\n",
    "    acc_map = torch.sum(weights, -1) \n",
    "    rgb_map = torch.sum(weights.unsqueeze(-1) * rgb, -2)\n",
    "\n",
    "    # Add White Background\n",
    "    rgb_map = rgb_map + (1. - acc_map.unsqueeze(-1) )\n",
    "    \n",
    "    return rgb_map,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20efa332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pdf(bins, weights, N_fine=128, noise=True):\n",
    "    \"\"\"\n",
    "    Sample N_fine points from the probability distribution defined by weights.\n",
    "    bins: [Batch, N_coarse-1] (Mid-points of coarse z_vals)\n",
    "    weights: [Batch, N_coarse-2] (Weights from coarse network)\n",
    "    \"\"\"\n",
    "    weights = weights + 1e-5\n",
    "    pdf = weights / torch.sum(weights, -1, keepdim=True)\n",
    "    \n",
    "    cdf = torch.cumsum(pdf, -1)\n",
    "    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)  # [Batch, N_coarse-1]\n",
    "    bins = torch.cat([bins, bins[..., -1:]], -1) \n",
    "\n",
    "    # 3. Generate random queries (u)\n",
    "    shape_of_queries= list(cdf.shape[:-1]) + [N_fine]\n",
    "    # (Batch,128)\n",
    "    if noise:\n",
    "        u = torch.rand(shape_of_queries, device=weights.device)\n",
    "    else:\n",
    "        u = torch.linspace(0., 1., steps=N_fine, device=weights.device)\n",
    "        u = u.expand(shape_of_queries)\n",
    "\n",
    "    inds = torch.searchsorted(cdf, u.contiguous(), right=True)\n",
    "    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n",
    "    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n",
    "\n",
    "    inds_g = torch.stack([below, above], -1)\n",
    "\n",
    "    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n",
    "    # [4096, 128, 64]\n",
    "    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
    "    bins_g = torch.gather(bins.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
    "    # cdf_g[..., 0] = CDF Low \n",
    "    # cdf_g[..., 1] = CDF High \n",
    "   \n",
    "    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n",
    "    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "    t = (u - cdf_g[..., 0]) / denom\n",
    "    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ae513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hierarchical():\n",
    "    criterion = nn.MSELoss()\n",
    "    coarse.train()\n",
    "    fine.train()\n",
    "    \n",
    "    loss_history = []\n",
    "    epoch_bar = tqdm(range(num_epochs), desc=\"Epochs\")\n",
    "    \n",
    "    for epoch in epoch_bar:\n",
    "        total_loss = 0\n",
    "        \n",
    "        for rays_o, rays_d, targets in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            rays_o, rays_d, targets = rays_o.to(device), rays_d.to(device), targets.to(device)\n",
    "            \n",
    "            N_c = 64\n",
    "            N_f = 128\n",
    "            near, far = 2.0, 6.0\n",
    "            \n",
    "            z_vals = torch.linspace(near, far, steps=N_c, device=device)\n",
    "            z_vals = z_vals.repeat(rays_o.shape[0], 1)\n",
    "\n",
    "            m_rand = (torch.rand(z_vals.shape, device=device) - 0.5) * (far - near) / N_c\n",
    "            z_vals = z_vals + m_rand\n",
    "\n",
    "            pts = rays_o.unsqueeze(1) + rays_d.unsqueeze(1) * z_vals.unsqueeze(-1)\n",
    "            \n",
    "            pts_flat = pts.reshape(-1, 3)\n",
    "            dirs_flat = rays_d.unsqueeze(1).expand(pts.shape).reshape(-1, 3)\n",
    "            \n",
    "            sigma_c, rgb_c = coarse(pts_flat, dirs_flat)\n",
    "            raw_c = torch.cat([rgb_c, sigma_c], -1).reshape(rays_o.shape[0], N_c, 4)\n",
    "            \n",
    "            rgb_map_c, weights_c = volume_render(raw_c, z_vals, rays_d)\n",
    "\n",
    "       \n",
    "            z_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "            \n",
    "            z_samples = sample_pdf(z_vals_mid, weights_c[..., :-1], N_f)\n",
    "            z_samples = z_samples.detach() \n",
    "            \n",
    "            z_vals_fine, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n",
    "            \n",
    "            pts_fine = rays_o.unsqueeze(1) + rays_d.unsqueeze(1) * z_vals_fine.unsqueeze(-1)\n",
    "            \n",
    "            pts_flat_f = pts_fine.reshape(-1, 3)\n",
    "            dirs_flat_f = rays_d.unsqueeze(1).expand(pts_fine.shape).reshape(-1, 3)\n",
    "            \n",
    "            sigma_f, rgb_f = fine(pts_flat_f, dirs_flat_f)\n",
    "            raw_f = torch.cat([rgb_f, sigma_f], -1).reshape(rays_o.shape[0], N_c + N_f, 4)\n",
    "            \n",
    "            rgb_map_f, _ = volume_render(raw_f, z_vals_fine, rays_d)\n",
    "\n",
    "            loss = criterion(rgb_map_c, targets) + criterion(rgb_map_f, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        loss_history.append(avg_train_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            coarse.eval()\n",
    "            fine.eval()\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for rays_o, rays_d, targets in val_loader:\n",
    "                    rays_o, rays_d, targets = rays_o.to(device), rays_d.to(device), targets.to(device)\n",
    "                    \n",
    "                    # Coarse Pass\n",
    "                    N_c, near, far = 64, 2.0, 6.0\n",
    "                    t_vals = torch.linspace(0., 1., steps=N_c, device=device)\n",
    "                    z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "                    z_vals = z_vals.expand([rays_o.shape[0], N_c])\n",
    "                    \n",
    "                    pts = rays_o.unsqueeze(-2)+ rays_d.unsqueeze(-2)* z_vals.unsqueeze(-1)\n",
    "                    pts_flat = pts.reshape(-1, 3)\n",
    "                    dirs_flat = rays_d[:, None, :].expand(pts.shape).reshape(-1, 3)\n",
    "                    sigma_c, rgb_c = coarse(pts_flat, dirs_flat)\n",
    "                    raw_c = torch.cat([rgb_c, sigma_c], -1).reshape(rays_o.shape[0], N_c, 4)\n",
    "                    rgb_map_c, weights_c = volume_render(raw_c, z_vals, rays_d)\n",
    "                    \n",
    "                    N_f = 128\n",
    "                    z_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "                    z_samples = sample_pdf(z_vals_mid, weights_c[..., :-1], N_f, noise=False) \n",
    "                    z_vals_fine, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n",
    "                    \n",
    "                    pts_fine = rays_o.unsqueeze(-2)+ rays_d.unsqueeze(-2)* z_vals_fine.unsqueeze(-1)\n",
    "                    sigma_f, rgb_f = fine(pts_fine.reshape(-1, 3), rays_d[:, None, :].expand(pts_fine.shape).reshape(-1, 3))\n",
    "                    raw_f = torch.cat([rgb_f, sigma_f], -1).reshape(rays_o.shape[0], N_c + N_f, 4)\n",
    "                    rgb_map_f, _ = volume_render(raw_f, z_vals_fine, rays_d)\n",
    "                    \n",
    "                    loss = criterion(rgb_map_f, targets)\n",
    "                    total_val_loss += loss.item()\n",
    "                torch.save({\n",
    "                    'coarse': coarse.state_dict(),\n",
    "                    'fine': fine.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }, 'nerf_checkpoint.pth')\n",
    "                print(\"\\nMODEL SAVED\\n\")\n",
    "\n",
    "            val_psnr = -10. * torch.log10(torch.tensor(total_val_loss / len(val_loader)))\n",
    "            epoch_bar.set_postfix({\"Loss\": f\"{avg_train_loss:.4f}\", \"Val PSNR\": f\"{val_psnr:.2f}\"})\n",
    "\n",
    "    return loss_history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc4b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = train_hierarchical()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Training Loss (MSE)\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e59f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    trans_t = lambda t : torch.Tensor([\n",
    "        [1,0,0,0],\n",
    "        [0,1,0,0],\n",
    "        [0,0,1,t],\n",
    "        [0,0,0,1]]).float()\n",
    "\n",
    "    rot_phi = lambda phi : torch.Tensor([\n",
    "        [1,0,0,0],\n",
    "        [0,np.cos(phi),-np.sin(phi),0],\n",
    "        [0,np.sin(phi), np.cos(phi),0],\n",
    "        [0,0,0,1]]).float()\n",
    "\n",
    "    rot_theta = lambda th : torch.Tensor([\n",
    "        [np.cos(th),0,-np.sin(th),0],\n",
    "        [0,1,0,0],\n",
    "        [np.sin(th),0, np.cos(th),0],\n",
    "        [0,0,0,1]]).float()\n",
    "    \n",
    "    c2w = trans_t(radius)\n",
    "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
    "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
    "    \n",
    "    c2w = torch.Tensor(np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]])) @ c2w\n",
    "    return c2w\n",
    "def render_minimal(checkpoint_path, save_path='./video.mp4'):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Rendering on {device}...\")\n",
    "\n",
    "    coarse = NeRF(L1=10, L2=4).to(device)\n",
    "    fine = NeRF(L1=10, L2=4).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    coarse.load_state_dict(checkpoint['coarse'])\n",
    "    fine.load_state_dict(checkpoint['fine'])\n",
    "    \n",
    "    coarse.eval()\n",
    "    fine.eval()\n",
    "\n",
    "    test_dataset = LegoRayDataset(split='test')\n",
    "    H, W = 800, 800\n",
    "    focal = test_dataset.fov\n",
    "    \n",
    "    poses = [pose_spherical(angle, -30.0, 4.0) for angle in np.linspace(0, 360, 40)[:-1]]\n",
    "    frames = []\n",
    "\n",
    "    for c2w in tqdm(poses):\n",
    "        c2w = c2w.to(device)\n",
    "        rays_o, rays_d = get_rays(H, W, focal, c2w)\n",
    "        rays_o, rays_d = rays_o.reshape(-1, 3), rays_d.reshape(-1, 3)\n",
    "        \n",
    "        chunk_size = 4096 \n",
    "        img_flat = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, rays_o.shape[0], chunk_size):\n",
    "                bo = rays_o[i:i+chunk_size]\n",
    "                bd = rays_d[i:i+chunk_size]\n",
    "                \n",
    "                N_c = 64\n",
    "                t = torch.linspace(0., 1., steps=N_c, device=device)\n",
    "                z = 2.0 * (1.-t) + 6.0 * t\n",
    "                z = z.expand([bo.shape[0], N_c])\n",
    "                \n",
    "                pts = bo.unsqueeze(1) + bd.unsqueeze(1) * z.unsqueeze(-1)\n",
    "                \n",
    "                pts_flat = pts.reshape(-1, 3)\n",
    "                dirs_flat = bd.unsqueeze(1).expand(pts.shape).reshape(-1, 3)\n",
    "                \n",
    "                sigma_c, rgb_c = coarse(pts_flat, dirs_flat)\n",
    "                raw_c = torch.cat([rgb_c, sigma_c], -1).reshape(bo.shape[0], N_c, 4)\n",
    "                \n",
    "                _, w_c = volume_render(raw_c, z, bd)\n",
    "                \n",
    "                N_f = 128\n",
    "                z_mid = .5 * (z[..., 1:] + z[..., :-1])\n",
    "                z_samp = sample_pdf(z_mid, w_c[..., :-1], N_f, noise=False)\n",
    "                z_fine, _ = torch.sort(torch.cat([z, z_samp], -1), -1)\n",
    "                \n",
    "                pts_f = bo.unsqueeze(1) + bd.unsqueeze(1) * z_fine.unsqueeze(-1)\n",
    "                \n",
    "                pts_flat_f = pts_f.reshape(-1, 3)\n",
    "                dirs_flat_f = bd.unsqueeze(1).expand(pts_f.shape).reshape(-1, 3)\n",
    "                \n",
    "                sigma_f, rgb_f = fine(pts_flat_f, dirs_flat_f)\n",
    "                raw_f = torch.cat([rgb_f, sigma_f], -1).reshape(bo.shape[0], N_c + N_f, 4)\n",
    "                \n",
    "                rgb, _ = volume_render(raw_f, z_fine, bd)\n",
    "                img_flat.append(rgb.cpu())\n",
    "\n",
    "        img = torch.cat(img_flat, 0).reshape(H, W, 3)\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        frames.append((img.numpy() * 255).astype(np.uint8))\n",
    "\n",
    "    imageio.mimsave(save_path, frames, fps=30, quality=8)\n",
    "    print(f\"Video saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584170ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_minimal('./nerf_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd25b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
